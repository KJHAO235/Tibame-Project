import os
import re
import sys
import json
import timeit
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from configparser import ConfigParser
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from pymongo import MongoClient
from bson import ObjectId  # ç¢ºä¿æ­£ç¢ºè™•ç† MongoDB çš„ ObjectId

# ç¦ç”¨ TensorFlow çš„ oneDNN å„ªåŒ–
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
# ç¦ç”¨ Transformers çš„ TensorFlow åŠ è¼‰
os.environ["TRANSFORMERS_NO_TF"] = "1"

try:
    config = ConfigParser()
    config.read("config.ini", encoding="utf-8")
    DEPLOYMENT_NAME_EMBEDDING_LARGE = config.get("AzureOpenAI", "DEPLOYMENT_NAME_EMBEDDING_LARGE")
    VERSION = config.get("AzureOpenAI", "VERSION")
    KEY = config.get("AzureOpenAI", "KEY")
    ENDPOINT = config.get("AzureOpenAI", "ENDPOINT")
    GPT4o_DEPLOYMENT_NAME = config.get("AzureOpenAI", "GPT4o_DEPLOYMENT_NAME")
    if not DEPLOYMENT_NAME_EMBEDDING_LARGE or not VERSION or not KEY or not ENDPOINT or not GPT4o_DEPLOYMENT_NAME:
        raise ValueError("Config file variables not set properly")
except Exception as e:
    print(f"Failed to load config file: {e}")
    try:
        DEPLOYMENT_NAME_EMBEDDING_LARGE = os.getenv("DEPLOYMENT_NAME_EMBEDDING_LARGE")
        VERSION = os.getenv("VERSION")
        KEY = os.getenv("KEY")
        ENDPOINT = os.getenv("ENDPOINT")
        GPT4o_DEPLOYMENT_NAME = os.getenv("GPT4o_DEPLOYMENT_NAME")
    except Exception as e:
        print(f"Failed to load config file: {e}")
        print("Please set the environment variables or provide a valid config.ini file.")
        sys.exit(1)


# 1. åŠ è¼‰æ•¸æ“šé›†
def load_dataset_from_mongodb_collection(collection_name):
    """å¾ MongoDB çš„ç‰¹å®šé›†åˆä¸­åŠ è¼‰å» å•†è³‡æ–™ã€‚

    Args:
        collection_name (str): è¦çˆ¬å–æ–‡æª”çš„é›†åˆåç¨±ã€‚

    Returns:
        list: åŒ…å«æ–‡æª”çš„ Document å°è±¡åˆ—è¡¨ã€‚
    """
    client = MongoClient("mongodb+srv://jenhao:abcd1234@cluster0.0vluw.mongodb.net/")
    # åˆ‡æ›åˆ°ç‰¹å®šè³‡æ–™åº«
    db_cp = client["companyDB"]

    # ç¢ºèªé›†åˆåç¨±æ˜¯å¦å­˜åœ¨
    collections = db_cp.list_collection_names()  # ç²å–è³‡æ–™åº«ä¸­çš„æ‰€æœ‰é›†åˆåç¨±
    # print(f"ç™¼ç¾é›†åˆ: {collections}")  # èª¿è©¦æ‰“å°é›†åˆåç¨±

    if collection_name not in collections:
        print(f"é›†åˆ {collection_name} ä¸å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­ï¼")
        return []

    # é¸æ“‡ç‰¹å®šé›†åˆ
    collection = db_cp[collection_name]

    documents = []
    for record in collection.find():
        # æå–æ‰€éœ€å­—æ®µ
        info = (record.get("info") or "").strip()
        doc_type = (record.get("type") or "").strip()
        name = (record.get("name") or "").strip()
        # æª¢æŸ¥ info æ˜¯å¦åŒ…å« "å» å•†åˆ†é¡ï¼š" + {type}
        match_phrase = f"å» å•†åˆ†é¡ï¼š{doc_type}"
        if not info or match_phrase in info:
            page_content = f"{name}\n{doc_type}".strip()  # è‹¥åŒ¹é…å‰‡æ”¹ç”¨å» å•†åç¨±
        else:
            page_content = f"{info}\n{doc_type}".strip()  # å¦å‰‡ä¿ç•™åŸ info

        # æ·»åŠ åˆ°æ–‡æª”åˆ—è¡¨
        documents.append(Document(
            page_content=page_content,
            metadata={
                "_id": record.get("_id", ""),
                "name": name,
                "id": record.get("id", ""),
                "type": doc_type,
                "collection": collection_name,  # æ·»åŠ é›†åˆåç¨±ä½œç‚ºä¾†æºå…ƒæ•¸æ“š
            }
        ))
    # print(f"åŠ è¼‰æ–‡æª”:{documents}")
    print(f"å¾é›†åˆ {collection_name} å…±åŠ è¼‰äº† {len(documents)} æ¢è³‡æ–™ã€‚")
    return documents

# 4. è¼¸å…¥åˆ†é¡èˆ‡åŒ¹é…
def classify_and_complete_input(user_input, processed_documents):
    """åˆ†é¡ç”¨æˆ¶è¼¸å…¥ï¼Œä¸¦è¿”å›åŒ¹é…çš„çµæœã€‚

    Args:
        user_input (str): ç”¨æˆ¶è¼¸å…¥çš„æ–‡å­—ã€‚
        processed_documents (list): è™•ç†å¾Œçš„ Document åˆ—è¡¨ã€‚

    Returns:
        tuple: åŒ…å«è¼¸å…¥é¡å‹å’ŒåŒ¹é…ä¿¡æ¯çš„å…ƒçµ„ã€‚
    """
    # å„ªå…ˆåŒ¹é…æ”¤ä½ç·¨è™Ÿ
    for doc in processed_documents:
        if "id" in doc.metadata:
            id_value = doc.metadata.get("id", "").strip().upper()
            if id_value == user_input.strip().upper():
                return "store", doc

    # åŒ¹é…éƒ¨åˆ†å…¬å¸åç¨±
    matched_stores = [
        doc for doc in processed_documents
        if user_input in doc.metadata.get("name", "")
    ]

    # å»é‡ï¼Œæ ¹æ“š id ä¾†ç¢ºä¿å”¯ä¸€æ€§
    unique_stores = {doc.metadata.get("id", ""): doc for doc in matched_stores}

    # åˆ¤å®šéæ–¼å»£æ³›
    if len(unique_stores) > 5:
        return "too_many_matches", list(unique_stores.values())

    # ç²¾æº–åŒ¹é…å–®å€‹å…¬å¸
    if len(unique_stores) == 1:
        return "store", list(unique_stores.values())[0]

    # é»˜èªç‚ºæè¿°é¡å‹
    return "object", user_input

# 5. æ ¹æ“šè¼¸å…¥é¡å‹é€²è¡Œæ¨è–¦
def recommend_based_on_input(input_type, matching_info, user_input, db, k=4):
    """æ ¹æ“šåˆ†é¡çµæœé€²è¡Œæ¨è–¦ã€‚
    Args:
        input_type (str): è¼¸å…¥é¡å‹ï¼Œå¯ç‚º "store", "too_many_matches", "object"ã€‚
        matching_info: åŒ¹é…çš„ç›¸é—œè³‡æ–™ï¼Œå¯èƒ½æ˜¯å–®å€‹åº—å®¶æˆ–å¤šå€‹åº—å®¶åˆ—è¡¨ã€‚
        user_input (str): ç”¨æˆ¶è¼¸å…¥ã€‚
        db (FAISS): å‘é‡ç´¢å¼•å°è±¡ã€‚
        k (int): è¿”å›çš„æ¨è–¦çµæœæ•¸é‡ã€‚
    """
    if input_type == "too_many_matches":
        print("è¼¸å…¥éæ–¼å»£æ³›ï¼Œè«‹é‡æ–°è¼¸å…¥æ›´ç²¾ç¢ºçš„å…§å®¹ã€‚")
        return
    if input_type == "store":
        query = f"I like {matching_info.metadata.get('name', 'æœªçŸ¥åº—å®¶åç¨±')}ï¼ŒWhich stores could you suggest to me?"
        results = db.similarity_search_with_score(query, k=k * 2)
        grouped_results = {}
        for doc, score in results:
            store_name = doc.metadata.get("name", "æœªçŸ¥åº—å®¶")
            if store_name not in grouped_results or grouped_results[store_name]["score"] > score:
                grouped_results[store_name] = {"doc": doc, "score": score}
        return sorted(grouped_results.values(), key=lambda x: x["score"])[:k]
    elif input_type == "object":
        query = f"I like {user_input} ï¼ŒWhich stores could you suggest to me?"
    results = db.similarity_search_with_score(query, k=k * 2)
    grouped_results = {}
    for doc, score in results:
        store_name = doc.metadata.get("name", "æœªçŸ¥åº—å®¶")
        if store_name not in grouped_results or grouped_results[store_name]["score"] > score:
            grouped_results[store_name] = {"doc": doc, "score": score}
    return sorted(grouped_results.values(), key=lambda x: x["score"])[:k]

# 6. åŠ è¼‰ LLM æ¨¡å‹å’Œ prompt
def initialize_llm():
    """åˆå§‹åŒ– LLM æ¨¡å‹å’Œæç¤ºæ¨¡æ¿ã€‚

    Returns:
        StuffDocumentsChain: æ–‡æª”è™•ç†éˆã€‚
    """
    llm = AzureChatOpenAI(
        azure_deployment=GPT4o_DEPLOYMENT_NAME,
        openai_api_version=VERSION,
        api_key=KEY,
        azure_endpoint=ENDPOINT,
    )
    prompt = ChatPromptTemplate.from_template(
        """ You are a helpful guide to recommend exhibitions.
        Answer the following question based only on the provided exhibitions context:
        <context>
        {context}
        </context>
        Question: {input}
        Based on the user's interest in "{input}", recommend 3 stores from the provided context.
        Your recommendations should:
        1.The recommendations have similar points.
        2.Include the store name and unique highlights in **a single sentence with no more than 50 words**.
        3.Translate all responses into Traditional Chinese.
        Please use the following structure for your answer:
        1. åº—å®¶åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        2. åº—å®¶åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        3. åº—å®¶åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        4. åº—å®¶åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        è‹¥ç„¡ç›¸é—œé›·åŒé»æ™‚ï¼Œæè¿°ç‚ºï¼šå¯èƒ½æœƒæœ‰[èˆ‡è©²åº—å®¶çš„é—œè¯æ€§èªªæ˜]ï¼Œç„¶å¾Œä»‹ç´¹è©²åº—å®¶çš„äº®é»ã€‚ 
        """
    )
    #in one sentences
    print("LLM æ¨¡å‹å·²åˆå§‹åŒ–å®Œæˆã€‚")
    return create_stuff_documents_chain(llm, prompt)

# 7. ç”Ÿæˆ LLM æ¨è–¦çµæœ
def generate_llm_recommendations(chain, query, context_docs):
    """ç”ŸæˆåŸºæ–¼ LLM çš„æ¨è–¦çµæœã€‚

    Args:
        chain (StuffDocumentsChain): æ–‡æª”è™•ç†éˆã€‚
        query (str): æŸ¥è©¢å­—ç¬¦ä¸²ã€‚
        context_docs (list): ä¸Šä¸‹æ–‡æ–‡æª”åˆ—è¡¨ã€‚

    Returns:
        str: LLM æ¨è–¦çš„ç­”æ¡ˆã€‚
    """
    context_docs = [
        Document(
            page_content=f"åº—å®¶åç¨±: {doc.metadata['name']}\n"
                         f"å…§å®¹: {doc.page_content}",
            metadata=doc.metadata
        )
        for doc in context_docs
    ]
    print(f"ç”Ÿæˆæ¨è–¦ä¸­context_docsï¼š{context_docs}")
    # ç”Ÿæˆæ¨è–¦ä¸­context_docsï¼š[Document(metadata={'_id': ObjectId('677c94b6cd8df06c18b4f096'), 'name': 'è–æ¯”å¾·è›‹ç³•æœ‰é™å…¬å¸', 'id': 'å°šæœªæ›´æ–°', 'type': 'ååº—è¡—', 'collection': '677c94b5cd8df06c18b4f021', 'chunk_index': 0}, page_content='åº—å®¶åç¨±: è–æ¯”å¾·è›‹ç³•æœ‰é™å…¬å¸\nå…§å®¹: è–æ¯”å¾·è›‹ç³•æœ‰é™å…¬å¸\nååº—è¡—')]
    print(f"ç”Ÿæˆæ¨è–¦ä¸­queryï¼š{query}")
    # èª¿ç”¨ chain.invoke æ–¹æ³•ï¼Œç¢ºä¿å‚³éäº† input å’Œ context å±¬æ€§
    return chain.invoke({"input": query, "context": context_docs})

def parse_llm_response(llm_result: str) -> dict:
    """
    å¾ LLM å›ç­”ä¸­ï¼Œè§£æå‡ºã€Œå±•è¦½åç¨± -> äº®é»ã€çš„å­—å…¸æ˜ å°„ã€‚
    
    Args:
        llm_result (str): LLM çš„æ–‡å­—å›ç­”
    
    Returns:
        highlight_map (dict): { '2025 å°åŒ—åœ‹éš›è‡ªå‹•åŒ–å·¥æ¥­å¤§å±•': 'èšç„¦å·¥æ¥­4.0...', 
                                '2025 æ™ºæ…§é¡¯ç¤ºå±•è¦½æœƒ': 'å±•ç¤ºæ™ºæ…§é¡¯ç¤º...', 
                                '2025 å°ç£æ©Ÿå™¨äººèˆ‡æ™ºæ…§è‡ªå‹•åŒ–å±•': 'åŒ¯é›†å…¨çƒ...' }
    """
    # ä½ ä¹Ÿå¯ä»¥ä¾å¯¦éš› LLM å›ç­”æ ¼å¼èª¿æ•´ regex
    # å‡è¨­æ ¼å¼å¤§è‡´ç‚ºï¼š
    # 1. å±•è¦½åç¨±: XXXXX
    #    ç¨ç‰¹äº®é»: YYYYY
    pattern = re.compile(
        r"\d+\.\s*åº—å®¶åç¨±:\s*(.*?)\s*ç¨ç‰¹äº®é»:\s*(.*)"
    )
    matches = pattern.findall(llm_result)

    highlight_map = {}
    for name, highlight in matches:
        # å»æ‰å‰å¾Œç©ºç™½
        exhibition_name = name.strip()
        exhibition_highlight = highlight.strip()
        highlight_map[exhibition_name] = exhibition_highlight

    return highlight_map

def save_llm_results(llm_result, context_docs):
    highlight_map = parse_llm_response(llm_result)

    doc_id_insert = {}
    # ğŸ”´ åŸæœ¬æ˜¯ for doc in context_docs[1:]:
    for doc in context_docs:  
        if "_id" in doc.metadata:
            _id = doc.metadata["_id"]
            name = doc.metadata.get("name", "æœªçŸ¥å±•è¦½")
            matched_highlight = highlight_map.get(name, "")
            # å¦‚æœ matched_highlight ç‚ºç©ºï¼Œä»£è¡¨ LLM æ²’æœ‰æä¾›å®ƒçš„äº®é»ï¼Œå°±ç›´æ¥è·³é
            if not matched_highlight:
                continue
            doc_id_insert[_id] = {
                "name": name,
                "highlight": matched_highlight
            }

    print("è§£æçµæœ:doc_id_insert ", doc_id_insert)
    print(f"è§£æçµæœï¼štype", type(doc_id_insert))
    return doc_id_insert

# def save_llm_results(llm_result, context_docs):
#     """è§£æ LLM æ¨è–¦çµæœä¸¦ä¿å­˜ç‚ºå­—å…¸ã€‚

#     Args:
#         llm_result (str): LLM çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
#         context_docs (list): èˆ‡æ¨è–¦ç›¸é—œçš„æ–‡æª”ä¸Šä¸‹æ–‡ã€‚

#     Returns:
#         dict: åŒ…å«æ¨è–¦çµæœçš„å­—å…¸ã€‚
#     """
#     parsed_results = {}
#     doc_id_insert = {
#         doc.metadata["_id"]: {"name": doc.metadata.get("name", "æœªçŸ¥åº—å®¶")}
#         for doc in context_docs[1:] if "_id" in doc.metadata
#     }
#     print(f"å»ºç«‹doc_id_insert: {doc_id_insert}")
#     highlight = [hl.split('ç¨ç‰¹äº®é»: ')[-1].strip() for hl in llm_result.split('\n\n')]
#     print(f"highlight: {highlight}")

#     # # ç§»é™¤èªªæ˜æ€§æ–‡å­—ï¼Œç¢ºä¿é«˜äº®èˆ‡åº—å®¶å°æ‡‰################################################################
#     # if len(highlight) > len(doc_id_insert):
#     #     highlight = highlight[1:]  # ç•¥éç¬¬ä¸€é …

#     for i, (doc_id, doc_info) in enumerate(doc_id_insert.items()):
#         doc_info["highlight"] = highlight[i]
    
#     print("è§£æçµæœ:doc_id_insert ", doc_id_insert)
#     print(f"è§£æçµæœï¼štype", type(doc_id_insert))
#     return doc_id_insert

# 8. ä¿å­˜æ¨è–¦çµæœ
# def save_llm_results(llm_result, context_docs):
#     """è§£æ LLM æ¨è–¦çµæœä¸¦ä¿å­˜ç‚ºå­—å…¸ã€‚

#     Args:
#         llm_result (str): LLM çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
#         context_docs (list): èˆ‡æ¨è–¦ç›¸é—œçš„æ–‡æª”ä¸Šä¸‹æ–‡ã€‚

#     Returns:
#         dict: åŒ…å«æ¨è–¦çµæœçš„å­—å…¸ã€‚
#     """
#     # æ§‹å»º doc_id_insert å­—å…¸
#     doc_id_insert = {
#         doc.metadata["_id"]: {"name": doc.metadata.get("name", "æœªçŸ¥åº—å®¶")}
#         for doc in context_docs if "_id" in doc.metadata
#     }
#     print(f"å»ºç«‹ doc_id_insert: {doc_id_insert}")

#     # ç”¨æ­£å‰‡è¡¨é”å¼æå–æ‰€æœ‰ã€Œç¨ç‰¹äº®é»ã€
#     # åŒ¹é…å½¢å¦‚ "ç¨ç‰¹äº®é»: ..." çš„å…§å®¹
#     highlight = re.findall(r'ç¨ç‰¹äº®é»: (.+)', llm_result)
#     print(f"æå–çš„ highlight: {highlight}")

#     # ç¢ºä¿ highlight èˆ‡ doc_id_insert çš„æ•¸é‡ä¸€è‡´
#     if len(highlight) != len(doc_id_insert):
#         raise ValueError(f"highlight çš„æ•¸é‡ ({len(highlight)}) èˆ‡ doc_id_insert ({len(doc_id_insert)}) ä¸åŒ¹é…ã€‚è«‹æª¢æŸ¥è¼¸å…¥æ•¸æ“šã€‚")

#     # åˆ†é… highlight åˆ° doc_id_insert
#     for i, (doc_id, doc_info) in enumerate(doc_id_insert.items()):
#         doc_info["highlight"] = highlight[i]
    
#     print("è§£æçµæœ: doc_id_insert ", doc_id_insert)
#     return doc_id_insert

# 9. åˆå§‹åŒ–ç’°å¢ƒ
def initialize_environment_store(current_collection, config_path="config.ini"):
    """åˆå§‹åŒ–ç’°å¢ƒï¼ŒåŒ…æ‹¬åµŒå…¥å‘é‡ã€FAISS è³‡æ–™åº«å’Œæ–‡æª”åŠ è¼‰ã€‚

    Args:
        current_collection (str): å±•è¦½é›†åˆåç¨±ã€‚
        config_path (str): é…ç½®æª”æ¡ˆè·¯å¾‘ã€‚

    Returns:
        tuple: (vector_stores, processed_documents_dict)
    """
    # åŠ è¼‰é…ç½®
    config = ConfigParser()
    config.read(config_path)

    # åˆå§‹åŒ–åµŒå…¥å‘é‡æ¨¡å‹
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=config["AzureOpenAI"]["DEPLOYMENT_NAME_EMBEDDING_LARGE"],
        openai_api_version=config["AzureOpenAI"]["VERSION"],
        api_key=config["AzureOpenAI"]["KEY"],
        azure_endpoint=config["AzureOpenAI"]["ENDPOINT"],
    )

    # æ§‹å»º FAISS è·¯å¾‘
    faiss_path = os.path.join("company_db_faiss", current_collection)

    # åŠ è¼‰ FAISS å‘é‡è³‡æ–™åº«
    try:
        prebuilt_faiss = FAISS.load_local(
            faiss_path,
            embeddings,
            "index",
            allow_dangerous_deserialization=True
        )
        print(f"æˆåŠŸåŠ è¼‰ FAISS è³‡æ–™åº«: {faiss_path}")
    except Exception as e:
        print(f"åŠ è¼‰ FAISS è³‡æ–™åº«å¤±æ•—: {e}")
        sys.exit(1)

    # åŠ è¼‰æ–‡æª”
    documents = load_dataset_from_mongodb_collection(current_collection)
    if not documents:
        print("æœªèƒ½åŠ è¼‰ä»»ä½•æ–‡æª”ï¼Œè«‹æª¢æŸ¥é›†åˆåç¨±æˆ–è³‡æ–™åº«é€£æ¥ã€‚")
        sys.exit(1)

    # æ§‹å»º vector_stores å’Œ processed_documents_dict
    vector_stores = {current_collection: prebuilt_faiss}
    processed_documents_dict = {current_collection: documents}
    
    return vector_stores, processed_documents_dict

# 10. ä½¿ç”¨è€…äº’å‹•è™•ç†
def user_interaction_store(current_collection, user_input, vector_stores, processed_documents_dict):
    """è™•ç†ä½¿ç”¨è€…è¼¸å…¥ï¼ŒåŸºæ–¼å·²çŸ¥çš„ç•¶å‰å±•è¦½é›†åˆé€²è¡Œæª¢ç´¢å’Œæ¨è–¦ã€‚

    Args:
        current_collection (str): ç•¶å‰å±•è¦½çš„é›†åˆåç¨±ã€‚
        vector_stores (dict): å‘é‡ç´¢å¼•å­—å…¸ï¼ŒåŒ…å«æ¯å€‹é›†åˆçš„å‘é‡ç´¢å¼•ã€‚
        processed_documents_dict (dict): æ–‡æª”åˆ—è¡¨å­—å…¸ï¼ŒåŒ…å«æ¯å€‹é›†åˆçš„è™•ç†å¾Œæ–‡æª”ã€‚
    """
    # é©—è­‰é›†åˆåç¨±æ˜¯å¦æœ‰æ•ˆ
    if current_collection not in vector_stores:
        print(f"ç„¡æ•ˆçš„é›†åˆåç¨±: {current_collection}")
        return {"result": f"ç„¡æ•ˆçš„é›†åˆåç¨±: {current_collection}"}

    # ç²å–å°æ‡‰é›†åˆçš„æ•¸æ“š
    db = vector_stores[current_collection]
    processed_documents = processed_documents_dict[current_collection]

    # æç¤ºç”¨æˆ¶è¼¸å…¥æŸ¥è©¢ï¼Œä¾‹å¦‚ï¼š
    # user_input = "é¦™å¸¥è›‹ç³•"

    # åˆ†é¡ç”¨æˆ¶è¼¸å…¥
    input_type, matching_info = classify_and_complete_input(user_input, processed_documents)

    # æ ¹æ“šåˆ†é¡é€²è¡Œæ¨è–¦
    if input_type == "too_many_matches":
        print(f"åŒ¹é…åˆ°çš„åº—å®¶è¶…é 5 å€‹ ({len(matching_info)} å€‹)ï¼Œè«‹é‡æ–°è¼¸å…¥æ›´ç²¾ç¢ºçš„å…§å®¹ã€‚")
        return {"result": "too_many_matches"}
    # æ ¹æ“šåˆ†é¡çµæœåŸ·è¡Œæª¢ç´¢æˆ–æ¨è–¦
    if input_type in ["store", "object"]:
        recommend_based_on_input(input_type, matching_info, user_input, db)
        print(f"æ ¹æ“šç”¨æˆ¶è¼¸å…¥é¡å‹ {input_type} é€²è¡Œæ¨è–¦ã€‚")

        # åˆå§‹åŒ– LLM ä¸¦ç”Ÿæˆæ¨è–¦çµæœ
        document_chain = initialize_llm()
        
        # åŸ·è¡Œåˆ†é¡èˆ‡æª¢ç´¢
        top_results = recommend_based_on_input(
            input_type,
            matching_info=matching_info,
            user_input=user_input,
            db=db,
            k=3
        )

        # å¦‚æœæ²’æœ‰æª¢ç´¢çµæœï¼Œé€€å‡º
        if not top_results:
            return {"result": "not top_resultsï¼Œæ²’æœ‰æª¢ç´¢çµæœ"}
        
        # æ‰“å°æª¢ç´¢çµæœ
        print("æª¢ç´¢çµæœï¼š")
        for result in top_results:
            doc = result["doc"]
            score = result["score"]
            print(f"åº—å®¶åç¨±: {doc.metadata['name']}")
            print(f"æ®µè½å…§å®¹: {doc.page_content}")
            print(f"ç›¸ä¼¼åº¦åˆ†æ•¸: {score}")
            print("-" * 50)
        # åˆå§‹åŒ– LLM ä¸¦ç”Ÿæˆæ¨è–¦çµæœ
        document_chain = initialize_llm()
        context_docs = [result["doc"] for result in top_results]
        print(f"é€™è£¡æ˜¯context_docsï¼š{context_docs}")
        llm_result = generate_llm_recommendations(document_chain, user_input, context_docs)


        # æ‰“å°ä¸¦ä¿å­˜ LLM æ¨è–¦çµæœ
        print("LLM Answer: ", llm_result)
        print('context_docs:', context_docs)
        return save_llm_results(llm_result, context_docs)

# 11. è¨ˆç®—è¨˜æ†¶é«”å¤§å°
def calculate_faiss_memory(vector_store):
    """è¨ˆç®— FAISS å‘é‡ç´¢å¼•çš„è¨˜æ†¶é«”ä½”ç”¨å¤§å°ã€‚

    Args:
        vector_store (FAISS): FAISS å‘é‡ç´¢å¼•å°è±¡ã€‚

    Returns:
        int: è¨˜æ†¶é«”ä½”ç”¨å¤§å°ï¼ˆä»¥ bytes ç‚ºå–®ä½ï¼‰ã€‚
    """
    index = vector_store.index  # ç²å– FAISS ç´¢å¼•
    dimension = index.d  # å‘é‡çš„ç¶­åº¦
    num_vectors = index.ntotal  # å‘é‡çš„æ•¸é‡
    vector_size = dimension * 4  # å‡è¨­æ¯å€‹æ•¸å€¼æ˜¯ float32ï¼ˆ4 bytesï¼‰

    total_size = num_vectors * vector_size
    return total_size


if __name__ == "__main__":
    # è¨ˆç®—åˆå§‹åŒ–åŸ·è¡Œæ™‚é–“
    # start_time = timeit.default_timer()
    # vector_stores, processed_documents_dict = backend_initialize_store()
    # end_time = timeit.default_timer()
    # execution_time = end_time - start_time
    # print(f"backend_initialize_store() åŸ·è¡Œæ™‚é–“: {execution_time:.4f} ç§’")

    # è¨ˆç®— FAISS å‘é‡ç´¢å¼•çš„è¨˜æ†¶é«”å¤§å°
    # total_memory = 0
    # for collection_name, vector_store in vector_stores.items():
    #     memory_size = calculate_faiss_memory(vector_store)
    #     total_memory += memory_size
    #     print(f"é›†åˆ {collection_name} çš„å‘é‡ç´¢å¼•è¨˜æ†¶é«”å¤§å°: {memory_size / (1024 ** 2):.2f} MB")

    # print(f"æ‰€æœ‰å‘é‡ç´¢å¼•çš„ç¸½è¨˜æ†¶é«”å¤§å°: {total_memory / (1024 ** 2):.2f} MB")

    # æŒ‡å®šç•¶å‰å±•è¦½çš„é›†åˆåç¨±ï¼Œè˜‡å“¥è«‹æ”¹é€™è£¡
    current_collection = "677c94b5cd8df06c18b4f021"
    user_input = "ç³•"

    # åˆå§‹åŒ–ç’°å¢ƒ
    vector_stores, processed_documents_dict = initialize_environment_store(current_collection)


    # ä½¿ç”¨è€…äº’å‹•ï¼Œæ ¹æ“šç•¶å‰å±•è¦½çš„é›†åˆé€²è¡Œæ“ä½œ
    a = user_interaction_store(current_collection, user_input, vector_stores, processed_documents_dict)
    print('++++++++++++++++++++++++++')
    print(a)
    # è¼¸å‡ºç¯„ä¾‹ï¼š
    # {ObjectId('677c94b6cd8df06c18b4f096'): {'name': 'è–æ¯”å¾·è›‹ç³•æœ‰é™å…¬å¸',
    # 'highlight': 'æä¾›å¤šç¨®å£å‘³çš„ç²¾ç·»è›‹ç³•ï¼Œä½æ–¼ååº—è¡—çš„çŸ¥åç”œé»åº—ã€‚'},
    # ObjectId('677c94b6cd8df06c18b4f0a1'): {'name': 'æ˜“å¾·é£Ÿé£Ÿå“æœ‰é™å…¬å¸',
    # 'highlight': 'ä»¥å¤šæ¨£åŒ–çš„ç³•é»é¸æ“‡èåï¼Œååº—è¡—ä¸­æ·±å—é¡§å®¢å–œæ„›ã€‚'},
    # ObjectId('677c94b6cd8df06c18b4f06c'): {'name': 'ä¸€ä¹‹è»’é£Ÿå“æœ‰é™å…¬å¸',
    # 'highlight': 'ä»¥é«˜å“è³ªçš„ç³•é»ç”¢å“è‘—ç¨±ï¼Œååº—è¡—çš„ç³•é»æ„›å¥½è€…å¿…è¨ªä¹‹åœ°ã€‚'},
    # ObjectId('677c94b6cd8df06c18b4f09e'): {'name': 'è¶…å“ä¼æ¥­è‚¡ä»½æœ‰é™å…¬å¸',
    # 'highlight': 'æä¾›å‰µæ–°å£å‘³çš„ç³•é»é¸æ“‡ï¼Œååº—è¡—ä¸Šçš„ç”œé»å‰µæ–°è€…ã€‚'}}

