import os
import json
import re
# from pprint import pprint
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from pymongo import MongoClient
import sys
from configparser import ConfigParser
from bson import ObjectId  # ç¢ºä¿æ­£ç¢ºè™•ç† MongoDB çš„ ObjectId

# åˆå§‹åŒ–é…ç½®è§£æå™¨
# config = ConfigParser()
# config.read("config.ini", encoding="utf-8")

try:
    DEPLOYMENT_NAME_EMBEDDING_LARGE = os.getenv("DEPLOYMENT_NAME_EMBEDDING_LARGE")
    VERSION = os.getenv("VERSION")
    KEY = os.getenv("KEY")
    ENDPOINT = os.getenv("ENDPOINT")
    GPT4o_DEPLOYMENT_NAME = os.getenv("GPT4o_DEPLOYMENT_NAME")

    if not all([DEPLOYMENT_NAME_EMBEDDING_LARGE, VERSION, KEY, ENDPOINT, GPT4o_DEPLOYMENT_NAME]):
        raise ValueError("One or more environment variables are missing.")
except Exception as e:
    print(f"Error loading environment variables: {e}")
    try:
        config = ConfigParser()
        config.read("config.ini", encoding="utf-8")

        DEPLOYMENT_NAME_EMBEDDING_LARGE = config.get("AzureOpenAI", "DEPLOYMENT_NAME_EMBEDDING_LARGE")
        VERSION = config.get("AzureOpenAI", "VERSION")
        KEY = config.get("AzureOpenAI", "KEY")
        ENDPOINT = config.get("AzureOpenAI", "ENDPOINT")
        GPT4o_DEPLOYMENT_NAME = config.get("AzureOpenAI", "GPT4o_DEPLOYMENT_NAME")
    except Exception as e:
        print(f"Error loading configuration file: {e}")
        sys.exit(1)
        




# ç¦ç”¨ TensorFlow çš„ oneDNN å„ªåŒ–
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
# ç¦ç”¨ Transformers çš„ TensorFlow åŠ è¼‰
os.environ["TRANSFORMERS_NO_TF"] = "1"


def load_dataset_from_mongodb() -> list:
    """å¾ MongoDB çš„ exhibitionDB çš„æ‰€æœ‰ collection åŠ è¼‰å±•è¦½è³‡æ–™ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰å±•è¦½æ•¸æ“šçš„ Document å½¢æˆçš„åˆ—è¡¨ã€‚
    """
    client = MongoClient("mongodb+srv://jenhao:abcd1234@cluster0.0vluw.mongodb.net/")
    db_ex = client["exhibitionDB"] 
    
    documents = []
    collections = db_ex.list_collection_names()  # ç²å–è³‡æ–™åº«ä¸­çš„æ‰€æœ‰é›†åˆåç¨±
    print(f"ç™¼ç¾é›†åˆ: {collections}")  # èª¿è©¦æ‰“å°é›†åˆåç¨±

    for collection_name in collections:
        # å°æ‰€æœ‰ collection é€²è¡Œè¿­ä»£ï¼Œæ¯å€‹ collection éƒ½æ˜¯ä¸€å€‹ï¼ˆå±•è¦½çš„ï¼‰ç¨®é¡ï¼Œä¾‹å¦‚ã€Œæ—…éŠå±•ã€
        collection = db_ex[collection_name]

        # å°æ¯å€‹ collection ä¸­çš„æ‰€æœ‰æ–‡æª”ï¼ˆé€™è£¡æ˜¯recordï¼Œå³ MongoDB ä¸­çš„ä¸€æ¢è¨˜éŒ„ï¼‰é€²è¡Œè¿­ä»£
        for record in collection.find(): 
            info = record.get("info", "").strip() # ç²å–å±•è¦½è³‡è¨Š
            # print(f"æª¢æŸ¥é›†åˆ {collection_name} çš„ info: {info}")  # æ‰“å° info

            documents.append(Document(
                    page_content=info,
                    metadata={
                        "_id": record.get("_id", ""),
                        "title": record.get("title", ""),
                        "date": record.get("date", ""),
                        "location": record.get("location", ""),
                        "collection": collection_name,  # æ·»åŠ é›†åˆåç¨±ä½œç‚ºä¾†æºå…ƒæ•¸æ“š
                        "info": info
                    }
                ))
    print(f"å¾ MongoDB åŠ è¼‰äº† {len(documents)} æ¢å±•è¦½è³‡æ–™ã€‚")
    return documents


# def process_documents(documents, chunk_size=75, chunk_overlap=10):
#     """å°‡æ–‡æª”é€²è¡Œæ–‡æœ¬åˆ‡å‰²ä¸¦è™•ç†ã€‚
#     Args:
#         documents (list): åŸå§‹æ–‡æª”åˆ—è¡¨ã€‚
#         chunk_size (int, optional): æ¯æ®µæ–‡å­—çš„å¤§å°ã€‚è¨­å®šç‚º 50ã€‚
#         chunk_overlap (int, optional): æ¯æ®µæ–‡å­—çš„é‡ç–Šå¤§å°ã€‚è¨­å®šç‚º 10ã€‚
#     Returns:
#         list: è™•ç†å¾Œçš„æ–‡æª”åˆ—è¡¨ã€‚
#     """
#     # åˆå§‹åŒ–æ–‡æœ¬åˆ‡å‰²å™¨
#     splitter = RecursiveCharacterTextSplitter(
#         chunk_size=chunk_size,
#         chunk_overlap=chunk_overlap,
#         length_function=len
#     )

#     processed_docs = []
#     for doc in documents: # documents æ˜¯ Documnet çš„listï¼Œ Documnet æ˜¯ langchain.schema ä¸­çš„ä¸€å€‹é¡
#         try:
#             # ç²å–æ–‡æª”å…§å®¹
#             info = doc.page_content.strip()
#             name = doc.metadata.get("title", "")
#             if not info:  # å¦‚æœå…§å®¹ç‚ºç©ºï¼Œç”¨å±•è¦½åç¨±æ›¿ä»£
#                 info = name
            
#             # åˆ‡å‰²æ–‡æœ¬
#             split_texts = splitter.split_text(info)
#             for idx, split_text in enumerate(split_texts):
#                 processed_docs.append(Document(
#                     page_content=split_text,
#                     metadata={
#                         "_id": str(doc.metadata.get('_id', 'æœªçŸ¥ _id')),
#                         "title": name,
#                         "date": doc.metadata.get("date", ""),
#                         "chunk_index": idx
#                     }
#                 ))
#         except Exception as e:
#             print(f"è™•ç†æ–‡æª”æ™‚å‡ºéŒ¯: {doc.page_content}, éŒ¯èª¤ä¿¡æ¯: {e}")
#             continue
#     return processed_docs

# # 3. åˆå§‹åŒ–å‘é‡ç´¢å¼•
# def initialize_faiss(processed_documents):
#     """åˆå§‹åŒ– FAISS å‘é‡ç´¢å¼•ã€‚
#     Args:
#         processed_documents (list): è™•ç†å¾Œçš„æ–‡æª”åˆ—è¡¨ã€‚
#     Returns:
#         FAISS: åˆå§‹åŒ–çš„å‘é‡ç´¢å¼•ã€‚
#     """
#     embeddings = AzureOpenAIEmbeddings(
#         azure_deployment=DEPLOYMENT_NAME_EMBEDDING_LARGE,
#         openai_api_version=VERSION,
#         api_key=KEY,
#         azure_endpoint=ENDPOINT,
#     )
#     print("å‘é‡ç´¢å¼•å·²åˆå§‹åŒ–å®Œæˆã€‚")
#     return FAISS.from_documents(processed_documents, embeddings)

# 4. æª¢ç´¢åŠŸèƒ½
def retrieve_top_by_exhibition(db, query, k=4):
    """æª¢ç´¢èˆ‡æŸ¥è©¢ç›¸é—œçš„å±•è¦½ã€‚
    Args:
        db (FAISS): å‘é‡ç´¢å¼•ã€‚
        query (str): æŸ¥è©¢å­—ç¬¦ä¸²ã€‚
        k (int, optional): è¿”å›çš„çµæœæ•¸é‡ã€‚é è¨­ç‚º 4ã€‚
    Returns:
        list: æœ€ç›¸é—œçš„å±•è¦½çµæœã€‚
    """
    results = db.similarity_search_with_score(query, k = k*2)
    grouped_results = {}
    for doc, score in results:
        exhibition = doc.metadata.get("title", "æœªçŸ¥å±•è¦½")
        if exhibition not in grouped_results or grouped_results[exhibition]["score"] > score:
            grouped_results[exhibition] = {"doc": doc, "score": score}
    
    print(f"æª¢ç´¢çµæœ: {grouped_results}")
    return sorted(grouped_results.values(), key=lambda x: x["score"])[:k]

# 5. æ ¹æ“š _id æŸ¥è©¢å±•è¦½æ¨™é¡Œé€²è¡Œåˆ‡æ›
def find_exhibition_title_by_id(documents, input_id):
    """
    æ ¹æ“šä½¿ç”¨è€…è¼¸å…¥çš„ MongoDB Atlas æ ¼å¼ _id æŸ¥è©¢å±•è¦½æ¨™é¡Œã€‚

    Args:
        documents (list): åŒ…å«å±•è¦½æ•¸æ“šçš„ Document åˆ—è¡¨ã€‚
        input_id (str): ä½¿ç”¨è€…è¼¸å…¥çš„ _id å­—ä¸²ã€‚

    Returns:
        str: å°æ‡‰çš„å±•è¦½æ¨™é¡Œï¼Œè‹¥æœªæ‰¾åˆ°å‰‡è¿”å›æç¤ºè¨Šæ¯ã€‚
    """
    for document in documents:
        # æå– _id çš„ $oid å€¼
        # document_id = document.metadata["_id"].get("$oid") if isinstance(document.metadata["_id"], dict) else None
        # if document_id == input_id:
        #     return document.metadata.get("title", "æœªæ‰¾åˆ°æ¨™é¡Œ")
        if str(document.metadata["_id"]) == input_id:
            return document.metadata.get("title", "æœªæ‰¾åˆ°æ¨™é¡Œ")
        # print(f"æœªæ‰¾åˆ°å°æ‡‰çš„å±•è¦½ï¼Œè«‹æª¢æŸ¥ _id æ˜¯å¦æ­£ç¢ºã€‚")
    return "æœªæ‰¾åˆ°å°æ‡‰çš„å±•è¦½"

# 6. åŠ è¼‰ LLM æ¨¡å‹å’Œ prompt
def initialize_llm():
    """åˆå§‹åŒ– LLM æ¨¡å‹å’Œæç¤ºæ¨¡æ¿ã€‚

    Returns:
        StuffDocumentsChain: æ–‡æª”è™•ç†éˆã€‚
    """
    llm = AzureChatOpenAI(
        azure_deployment=GPT4o_DEPLOYMENT_NAME,
        openai_api_version=VERSION,
        api_key=KEY,
        azure_endpoint=ENDPOINT,
    )
    prompt = ChatPromptTemplate.from_template(
        """ You are a helpful guide to recommend exhibitions.
        Answer the following question based only on the provided exhibitions context:
        <context>
        {context}
        </context>
        Question: {input}
        Based on the user's interest in "{input}", recommend 3 exhibitions from the provided context.
        Your recommendations should:
        1.The recommendations have similar points.
        2.Include the exhibition name and unique highlights in **a single sentence with no more than 50 words**..
        3.Translate all responses into Traditional Chinese.
        Please use the following structure for your answer:
        1. å±•è¦½åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        2. å±•è¦½åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        3. å±•è¦½åç¨±: [Name]
           ç¨ç‰¹äº®é»: [Highlights]
        """
    )
    #in one sentences
    print("LLM æ¨¡å‹å·²åˆå§‹åŒ–å®Œæˆã€‚")
    return create_stuff_documents_chain(llm, prompt)

# 7. ç”Ÿæˆ LLM æ¨è–¦çµæœ
def generate_llm_recommendations(chain, query, context_docs):
    """ç”ŸæˆåŸºæ–¼ LLM çš„æ¨è–¦çµæœã€‚

    Args:
        chain (StuffDocumentsChain): æ–‡æª”è™•ç†éˆã€‚
        query (str): æŸ¥è©¢å­—ç¬¦ä¸²ã€‚
        context_docs (list): ä¸Šä¸‹æ–‡æ–‡æª”åˆ—è¡¨ã€‚

    Returns:
        str: LLM æ¨è–¦çš„ç­”æ¡ˆã€‚
    """
    context_docs = [
        Document(
            page_content=f"å±•è¦½åç¨±: {doc.metadata.get('title', 'æœªçŸ¥å±•è¦½')}\n"
                         f"æ—¥æœŸ: {doc.metadata.get('date', 'æœªçŸ¥æ—¥æœŸ')}\n"
                         f"å…§å®¹: {doc.page_content}",
            metadata=doc.metadata
        )
        for doc in context_docs
    ]
    print(f"ç”Ÿæˆæ¨è–¦ä¸­{context_docs}")
    return chain.invoke({"input": query, "context": context_docs})

def parse_llm_response(llm_result: str) -> dict:
    """
    å¾ LLM å›ç­”ä¸­ï¼Œè§£æå‡ºã€Œå±•è¦½åç¨± -> äº®é»ã€çš„å­—å…¸æ˜ å°„ã€‚
    
    Args:
        llm_result (str): LLM çš„æ–‡å­—å›ç­”
    
    Returns:
        highlight_map (dict): { '2025 å°åŒ—åœ‹éš›è‡ªå‹•åŒ–å·¥æ¥­å¤§å±•': 'èšç„¦å·¥æ¥­4.0...', 
                                '2025 æ™ºæ…§é¡¯ç¤ºå±•è¦½æœƒ': 'å±•ç¤ºæ™ºæ…§é¡¯ç¤º...', 
                                '2025 å°ç£æ©Ÿå™¨äººèˆ‡æ™ºæ…§è‡ªå‹•åŒ–å±•': 'åŒ¯é›†å…¨çƒ...' }
    """
    # ä½ ä¹Ÿå¯ä»¥ä¾å¯¦éš› LLM å›ç­”æ ¼å¼èª¿æ•´ regex
    # å‡è¨­æ ¼å¼å¤§è‡´ç‚ºï¼š
    # 1. å±•è¦½åç¨±: XXXXX
    #    ç¨ç‰¹äº®é»: YYYYY
    pattern = re.compile(
        r"\d+\.\s*å±•è¦½åç¨±:\s*(.*?)\s*ç¨ç‰¹äº®é»:\s*(.*)"
    )
    matches = pattern.findall(llm_result)

    highlight_map = {}
    for name, highlight in matches:
        # å»æ‰å‰å¾Œç©ºç™½
        exhibition_name = name.strip()
        exhibition_highlight = highlight.strip()
        highlight_map[exhibition_name] = exhibition_highlight

    return highlight_map

def save_llm_results_as_dict(llm_result, context_docs):
    """è§£æ LLM æ¨è–¦çµæœä¸¦ä¿å­˜ç‚ºå­—å…¸ã€‚

    Args:
        llm_result (str): LLM çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
        context_docs (list): èˆ‡æ¨è–¦ç›¸é—œçš„æ–‡æª”ä¸Šä¸‹æ–‡ã€‚

    Returns:
        dict: åŒ…å«æ¨è–¦çµæœçš„å­—å…¸ã€‚
    """
    # ç¬¬ä¸€æ­¥ï¼šå…ˆç”¨å‰›æ‰å®šç¾©å¥½çš„å‡½å¼ï¼Œå–å¾—ã€Œå±•è¦½åç¨± -> highlightã€çš„ map
    highlight_map = parse_llm_response(llm_result)

    doc_id_insert = {}
    # ğŸ”´ åŸæœ¬æ˜¯ for doc in context_docs[1:]:
    for doc in context_docs:  
        if "_id" in doc.metadata:
            _id = doc.metadata["_id"]
            name = doc.metadata.get("title", "æœªçŸ¥å±•è¦½")
            matched_highlight = highlight_map.get(name, "")
            # å¦‚æœ matched_highlight ç‚ºç©ºï¼Œä»£è¡¨ LLM æ²’æœ‰æä¾›å®ƒçš„äº®é»ï¼Œå°±ç›´æ¥è·³é
            if not matched_highlight:
                continue
            doc_id_insert[_id] = {
                "title": name,
                "highlight": matched_highlight
            }

    print("è§£æçµæœ:doc_id_insert ", doc_id_insert)
    print(f"è§£æçµæœï¼štype", type(doc_id_insert))
    return doc_id_insert

    # ç¬¬äºŒæ­¥ï¼šå»ºç«‹ doc_id_insert çµæ§‹ï¼Œä¸¦ä¸”ä¾å±•è¦½åç¨±å°æ‡‰åˆ° highlight
    # doc_id_insert = {}
    # for doc in context_docs[1:]:  # ä½ çš„åŸå§‹ç¨‹å¼ä¸­å¥½åƒæ˜¯å¾ context_docs[1:] é–‹å§‹
    #     if "_id" in doc.metadata:
    #         _id = doc.metadata["_id"]
    #         title = doc.metadata.get("title", "æœªçŸ¥å±•è¦½")

    #         # æ ¹æ“šã€Œtitleã€åˆ° highlight_map æŸ¥æ‰¾å°æ‡‰çš„äº®é»
    #         matched_highlight = highlight_map.get(title, "")

    #         doc_id_insert[_id] = {
    #             "title": title,
    #             "highlight": matched_highlight
    #         }
    
    # print("è§£æçµæœ:doc_id_insert ", doc_id_insert)
    # print(f"è§£æçµæœï¼štype", type(doc_id_insert))
    # return doc_id_insert


# 8. ä¿å­˜æ¨è–¦çµæœç‚º dict
# def save_llm_results_as_dict(llm_result, context_docs):
#     """è§£æ LLM æ¨è–¦çµæœä¸¦ä¿å­˜ç‚ºå­—å…¸ã€‚

#     Args:
#         llm_result (str): LLM çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
#         context_docs (list): èˆ‡æ¨è–¦ç›¸é—œçš„æ–‡æª”ä¸Šä¸‹æ–‡ã€‚

#     Returns:
#         dict: åŒ…å«æ¨è–¦çµæœçš„å­—å…¸ã€‚
#     """
#     parsed_results = {}

#     # å»ºç«‹æ¨™é¡Œåˆ° _id çš„æ˜ å°„ï¼Œåƒ…è™•ç†åŒ…å« _id çš„æ–‡æª”
#     doc_id_insert = {
#         doc.metadata["_id"]: {"title": doc.metadata.get("title", "æœªçŸ¥å±•è¦½")}
#         for doc in context_docs[1:] if "_id" in doc.metadata
#     }
#     print(f"å»ºç«‹doc_id_insert: {doc_id_insert}")
    
#     highlight = [hl.split('ç¨ç‰¹äº®é»: ')[-1].strip() for hl in llm_result.split('\n\n')]
#     print(f"highlight: {highlight}")

#     # # ç§»é™¤èªªæ˜æ€§æ–‡å­—ï¼Œç¢ºä¿é«˜äº®èˆ‡åº—å®¶å°æ‡‰################################################################
#     if len(highlight) > len(doc_id_insert):
#         highlight = highlight[1:]  # ç•¥éç¬¬ä¸€é …

#     for i, (doc_id, doc_info) in enumerate(doc_id_insert.items()):
#         doc_info["highlight"] = highlight[i]
    
#     # print("è§£æçµæœ:parsed_results ", parsed_results)
#     # print(f"è§£æçµæœï¼š ", type(parsed_results))
#     print("è§£æçµæœ:doc_id_insert ", doc_id_insert)
#     print(f"è§£æçµæœï¼štype", type(doc_id_insert))
#     return doc_id_insert

# def save_llm_results_as_dict(llm_result, context_docs):
#     """è§£æ LLM æ¨è–¦çµæœä¸¦ä¿å­˜ç‚ºå­—å…¸ã€‚

#     Args:
#         llm_result (str): LLM çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
#         context_docs (list): èˆ‡æ¨è–¦ç›¸é—œçš„æ–‡æª”ä¸Šä¸‹æ–‡ã€‚

#     Returns:
#         dict: åŒ…å«æ¨è–¦çµæœçš„å­—å…¸ã€‚
#     """
#     # æ§‹å»º doc_id_insert å­—å…¸
#     doc_id_insert = {
#         doc.metadata["_id"]: {"title": doc.metadata.get("title", "æœªçŸ¥å±•è¦½")}
#         for doc in context_docs if "_id" in doc.metadata
#     }
#     print(f"å»ºç«‹ doc_id_insert: {doc_id_insert}")

#     # ç”¨æ­£å‰‡è¡¨é”å¼æå–æ‰€æœ‰ã€Œç¨ç‰¹äº®é»ã€
#     # åŒ¹é…å½¢å¦‚ "ç¨ç‰¹äº®é»: ..." çš„å…§å®¹
#     highlight = re.findall(r'ç¨ç‰¹äº®é»: (.+)', llm_result)
#     print(f"æå–çš„ highlight: {highlight}")

#     # ç¢ºä¿ highlight èˆ‡ doc_id_insert çš„æ•¸é‡ä¸€è‡´
#     if len(highlight) != len(doc_id_insert):
#         raise ValueError(f"highlight çš„æ•¸é‡ ({len(highlight)}) èˆ‡ doc_id_insert ({len(doc_id_insert)}) ä¸åŒ¹é…ã€‚è«‹æª¢æŸ¥è¼¸å…¥æ•¸æ“šã€‚")

#     # åˆ†é… highlight åˆ° doc_id_insert
#     for i, (doc_id, doc_info) in enumerate(doc_id_insert.items()):
#         doc_info["highlight"] = highlight[i]
    
#     print("è§£æçµæœ: doc_id_insert ", doc_id_insert)
#     return doc_id_insert

# def save_llm_results_as_dict(llm_result, context_docs):
#     """è§£æ LLM æ¨è–¦çµæœä¸¦ä¿å­˜ç‚ºå­—å…¸ã€‚

#     Args:
#         llm_result (str): LLM çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
#         context_docs (list): èˆ‡æ¨è–¦ç›¸é—œçš„æ–‡æª”ä¸Šä¸‹æ–‡ã€‚

#     Returns:
#         dict: åŒ…å«æ¨è–¦çµæœçš„å­—å…¸ã€‚
#     """
#     parsed_results = {}
#     doc_id_insert = {
#         doc.metadata["_id"]: {"name": doc.metadata.get("name", "æœªçŸ¥åº—å®¶")}
#         for doc in context_docs[1:] if "_id" in doc.metadata
#     }
#     print(f"å»ºç«‹doc_id_insert: {doc_id_insert}")
#     highlight = [hl.split('ç¨ç‰¹äº®é»: ')[-1].strip() for hl in llm_result.split('\n\n')]
#     print(f"highlight: {highlight}")

#     for i, (doc_id, doc_info) in enumerate(doc_id_insert.items()):
#         doc_info["highlight"] = highlight[i]
    
#     print("è§£æçµæœ:doc_id_insert ", doc_id_insert)
#     print(f"è§£æçµæœï¼štype", type(doc_id_insert))
#     return doc_id_insert

def initialize_environment_exh(config_path="config.ini"):
    """åˆå§‹åŒ–åµŒå…¥å‘é‡å’Œ FAISS è³‡æ–™åº«ã€‚

    Args:
        config_path (str): é…ç½®æª”æ¡ˆè·¯å¾‘ã€‚
        faiss_path (str): FAISS è³‡æ–™åº«çš„è·¯å¾‘ã€‚

    Returns:
        object: å·²åŠ è¼‰çš„ FAISS è³‡æ–™åº«ã€‚
    """
    # åŠ è¼‰é…ç½®
    config = ConfigParser()
    config.read(config_path)

    # åˆå§‹åŒ–åµŒå…¥å‘é‡æ¨¡å‹
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=config["AzureOpenAI"]["DEPLOYMENT_NAME_EMBEDDING_LARGE"],
        openai_api_version=config["AzureOpenAI"]["VERSION"],
        api_key=config["AzureOpenAI"]["KEY"],
        azure_endpoint=config["AzureOpenAI"]["ENDPOINT"],
    )

    # åŠ è¼‰ FAISS å‘é‡è³‡æ–™åº«

    faiss_path = os.path.join("exhibition_db_faiss")
    try:
        prebuilt_faiss = FAISS.load_local(
            faiss_path,
            embeddings,
            "index",
            allow_dangerous_deserialization=True
        )
        print(f"æˆåŠŸåŠ è¼‰ FAISS è³‡æ–™åº«: {faiss_path}")
        return prebuilt_faiss
    except Exception as e:
        print(f"åŠ è¼‰ FAISS è³‡æ–™åº«å¤±æ•—: {e}")
        raise

# ä½¿ç”¨è€…äº’å‹•è™•ç†
def user_interaction(db, latest_love_exhib_id):
    """è™•ç†ä½¿ç”¨è€…è¼¸å…¥ï¼Œé€²è¡Œæª¢ç´¢å’Œæ¨è–¦ã€‚
    Args:
        db (FAISS): å·²åˆå§‹åŒ–çš„å‘é‡ç´¢å¼•ã€‚
    """

    # æ ¹æ“šæˆ‘çš„æœ€æ„›æœ€æ–°ä¸€ç­†çš„_idï¼Œè˜‡å“¥è«‹æ”¹é€™è£¡
    # latest_love_exhib_id = "677c94b5cd8df06c18b4f021"

    # æŸ¥è©¢å±•è¦½æ¨™é¡Œ
    documents_db = load_dataset_from_mongodb()
    user_input = find_exhibition_title_by_id(documents_db, latest_love_exhib_id)
    print(f"å°æ‡‰çš„å±•è¦½æ¨™é¡Œ: {user_input}")

    # æ§‹å»ºæŸ¥è©¢
    query = f"I enjoy {user_input} ï¼ŒWhich exhibitions could you suggest to me?"
    
    # åŸ·è¡Œæª¢ç´¢
    top_results = retrieve_top_by_exhibition(db, query)
    
    # æ‰“å°æª¢ç´¢çµæœ
    print("æª¢ç´¢çµæœï¼š")
    for result in top_results:
        doc = result["doc"]
        score = result["score"]
        print(f"å±•è¦½åç¨±: {doc.metadata['title']}")
        print(f"æ®µè½å…§å®¹: {doc.page_content}")
        print(f"ç›¸ä¼¼åº¦åˆ†æ•¸: {score}")
        print("-" * 50)
    
    # åˆå§‹åŒ– LLM ä¸¦ç”Ÿæˆæ¨è–¦çµæœ
    document_chain = initialize_llm()
    llm_result = generate_llm_recommendations(document_chain, query, [r["doc"] for r in top_results]) #???
    print(f"å•é¡Œ{query}")

    # æ‰“å°ä¸¦ä¿å­˜ LLM æ¨è–¦çµæœ
    print("LLM Answer: ", llm_result)
    print("LLM Answer type: ", type(llm_result))
    return save_llm_results_as_dict(llm_result, [r["doc"] for r in top_results])


if __name__ == "__main__":
    # å¾Œå°åˆå§‹åŒ–
    # db = backend_initialize() #åˆå§‹åŒ–åŸ·è¡Œæ™‚é–“ç´„ç‚º 8 ç§’
    # æŸ¥çœ‹dbçš„ä½”è¨˜æ†¶é«”çš„å¤§å°
    # db_size = sys.getsizeof(db)
    # print(f"å‘é‡ç´¢å¼•ä½”ç”¨çš„è¨˜æ†¶é«”å¤§å°: {db_size} bytes")
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=config["AzureOpenAI"]["DEPLOYMENT_NAME_EMBEDDING_LARGE"],
        openai_api_version=config["AzureOpenAI"]["VERSION"],
        api_key=config["AzureOpenAI"]["KEY"],
        azure_endpoint=config["AzureOpenAI"]["ENDPOINT"],
    )
    # ä½¿ç”¨è€…äº’å‹•
    prebuilt_faiss=FAISS.load_local("exhibition_db_faiss", 
                                    embeddings, 
                                    "index",
                                    allow_dangerous_deserialization=True)
    print({prebuilt_faiss})
    
    # ä½¿ç”¨è€…äº’å‹•
    user_interaction(prebuilt_faiss, latest_love_exhib_id='') #è«‹æ”¹user_inputï¼ŒåŸ·è¡Œæ™‚é–“ç´„ç‚º 5 ç§’
    


